{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura\n",
    "\n",
    "Claves:\n",
    "1. Conexión con modelos\n",
    "2. Conexión con datos\n",
    "3. Encadenamiento de procesos\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/integrations/providers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinguillencisneros/Downloads/Developing_applications_with_Langchain/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# model = \"tiiuae/falcon-40b-instruct\" # Generacion de texto\n",
    "# model = \"stabilityai/stablelm-tuned-alpha-3b\"\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer= tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_generation.TextGenerationPipeline"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/s0pfsfdn61b_3c4kn01_hktc0000gn/T/ipykernel_5028/4016280788.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm_falcon = HuggingFacePipeline(\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm_falcon = HuggingFacePipeline(\n",
    "    pipeline = pipeline,\n",
    "    model_kwargs = {\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 10,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x16142d7f0>, model_kwargs={'temperature': 0, 'max_length': 200, 'do_sample': True, 'top_k': 10, 'num_return_sequences': 1, 'eos_token_id': 11})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/s0pfsfdn61b_3c4kn01_hktc0000gn/T/ipykernel_5028/1947423762.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_falcon(\"what is AI?\")\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what is AI?\\nAI stands for Artificial Intelligence. It is a branch of computer science that focuses on creating intelligent machines'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_falcon(\"what is AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models with openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPEN_AI_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_gpt3_5 = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    n = 1, #numero de diferentes respuestas\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x162c9d160>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x162ca9940>, root_client=<openai.OpenAI object at 0x1614c6e90>, root_async_client=<openai.AsyncOpenAI object at 0x162c9d2b0>, temperature=0.3, model_kwargs={}, openai_api_key=SecretStr('**********'), n=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gpt3_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Para lograr una clase más interactiva para estudiantes virtuales, puedes implementar las siguientes estrategias:\\n\\n1. Utilizar herramientas de videoconferencia que permitan la participación activa de los estudiantes, como la función de chat, la posibilidad de levantar la mano virtualmente para hacer preguntas o participar en la discusión, y la opción de compartir pantalla para mostrar trabajos o presentaciones.\\n\\n2. Fomentar la participación de los estudiantes a través de preguntas abiertas, encuestas en línea, debates virtuales y actividades colaborativas en tiempo real.\\n\\n3. Utilizar plataformas educativas interactivas que permitan a los estudiantes realizar actividades prácticas, como cuestionarios, juegos educativos, simulaciones y ejercicios de práctica.\\n\\n4. Incorporar recursos multimedia, como videos, infografías, podcasts y presentaciones interactivas, para mantener el interés de los estudiantes y facilitar la comprensión de los conceptos.\\n\\n5. Promover la colaboración entre los estudiantes a través de herramientas de trabajo en grupo en línea, como Google Docs, Microsoft Teams o Zoom breakout rooms, para que puedan trabajar juntos en proyectos y discutir ideas.\\n\\n6. Brindar retroalimentación constante a los estudiantes a través de comentarios personalizados, evaluaciones formativas y sesiones de tutoría individualizadas para ayudarles a mejorar su aprendizaje.\\n\\n7. Incentivar la participación activa de los estudiantes a través de la gamificación, como la creación de desafíos, competencias y premios virtuales para motivar su participación y compromiso con la clase.\\n\\nAl implementar estas estrategias, podrás crear una experiencia de aprendizaje más interactiva y enriquecedora para tus estudiantes virtuales, fomentando su participación, motivación y compromiso con el proceso de enseñanza-aprendizaje.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 22, 'total_tokens': 432, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BOxLHyTFfyRAfInLvGIMyIZwQ9Udl', 'finish_reason': 'stop', 'logprobs': None}, id='run-250b84b1-a65d-48cd-8fe9-c59a24e46122-0', usage_metadata={'input_tokens': 22, 'output_tokens': 410, 'total_tokens': 432, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gpt3_5.invoke('Como puedo lograr una clase mas interaciva para estudiantes virtuales?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_davinci = ChatOpenAI(\n",
    "    model_name = \"text-davinci-003\",\n",
    "    n = 2, #numero de diferentes respuestas\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generacion = llm_gpt3_5.generate(\n",
    "    [\n",
    "        \"Dime un consejo de vida para alguien de 30 años\",\n",
    "        \"Recomiendame libros similares a Hyperion Cantos\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatGeneration(text='Nunca es tarde para empezar de nuevo. A los 30 años, aún tienes mucho tiempo por delante para alcanzar tus metas y sueños. No te compares con los demás, sigue tu propio camino y trabaja duro para lograr lo que deseas. Aprovecha cada oportunidad que se presente y nunca dejes de aprender y crecer como persona. Recuerda que la vida es un viaje y lo importante es disfrutar el camino. ¡No te rindas y sigue adelante!', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Nunca es tarde para empezar de nuevo. A los 30 años, aún tienes mucho tiempo por delante para alcanzar tus metas y sueños. No te compares con los demás, sigue tu propio camino y trabaja duro para lograr lo que deseas. Aprovecha cada oportunidad que se presente y nunca dejes de aprender y crecer como persona. Recuerda que la vida es un viaje y lo importante es disfrutar el camino. ¡No te rindas y sigue adelante!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 20, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BOxLN59DpwbcL1sTssfrIH0sJ3YFM', 'finish_reason': 'stop', 'logprobs': None}, id='run-f72f0aaf-a1e0-4973-acb4-bb07f84e7948-0', usage_metadata={'input_tokens': 20, 'output_tokens': 110, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generacion.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatGeneration(text='1. \"Dune\" by Frank Herbert - A classic science fiction novel that explores themes of power, politics, and religion in a distant future.\\n\\n2. \"The Expanse series\" by James S.A. Corey - A series of science fiction novels that follow the crew of a spaceship as they navigate political intrigue and interstellar conflict.\\n\\n3. \"Altered Carbon\" by Richard K. Morgan - A cyberpunk novel that explores themes of identity, consciousness, and technology in a future where people can transfer their consciousness between bodies.\\n\\n4. \"The Culture series\" by Iain M. Banks - A series of science fiction novels that explore a post-scarcity society run by advanced artificial intelligences.\\n\\n5. \"The Commonwealth Saga\" by Peter F. Hamilton - A series of science fiction novels that follow humanity as they colonize the galaxy and encounter alien civilizations.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='1. \"Dune\" by Frank Herbert - A classic science fiction novel that explores themes of power, politics, and religion in a distant future.\\n\\n2. \"The Expanse series\" by James S.A. Corey - A series of science fiction novels that follow the crew of a spaceship as they navigate political intrigue and interstellar conflict.\\n\\n3. \"Altered Carbon\" by Richard K. Morgan - A cyberpunk novel that explores themes of identity, consciousness, and technology in a future where people can transfer their consciousness between bodies.\\n\\n4. \"The Culture series\" by Iain M. Banks - A series of science fiction novels that explore a post-scarcity society run by advanced artificial intelligences.\\n\\n5. \"The Commonwealth Saga\" by Peter F. Hamilton - A series of science fiction novels that follow humanity as they colonize the galaxy and encounter alien civilizations.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 176, 'prompt_tokens': 20, 'total_tokens': 196, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BOxLONXLr5FEURVIjoCaSg3SqIDFP', 'finish_reason': 'stop', 'logprobs': None}, id='run-5343ad75-0a5c-4e4a-9a8c-f0dadd4f1816-0', usage_metadata={'input_tokens': 20, 'output_tokens': 176, 'total_tokens': 196, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generacion.generations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 286,\n",
       "  'prompt_tokens': 40,\n",
       "  'total_tokens': 326,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generacion.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gpt3_5.get_num_tokens('Mis jefes se van a preocupar si gasto mucho en openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInstrucciones:\\nInformación externa o contexto:\\nEntrada del usuario o consulta:\\nIndicador de salida:\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Instrucciones:\n",
    "Información externa o contexto:\n",
    "Entrada del usuario o consulta:\n",
    "Indicador de salida:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/s0pfsfdn61b_3c4kn01_hktc0000gn/T/ipykernel_5028/1643221544.py:19: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm_gpt3_5(prompt_argentino))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ni idea, che.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 198, 'total_tokens': 204, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BOxLRynhRB6tfO5xWWJRgOVKUzg0d', 'finish_reason': 'stop', 'logprobs': None} id='run-7cc059bf-4bde-4ba8-926b-4a317af80e9e-0' usage_metadata={'input_tokens': 198, 'output_tokens': 6, 'total_tokens': 204, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "prompt_argentino = \"\"\" \n",
    "\n",
    "Respondé la pregunta basándote en el contexto de abajo. Si la\n",
    "pregunta no puede ser respondida usando la información proporcionada,\n",
    "respondé con \"Ni idea, che\".\n",
    "\n",
    "Contexto: Los Modelos de Lenguaje de Gran Escala (MLGEs) son lo último en modelos usados en el Procesamiento del Lenguaje Natural (NLP).\n",
    "Su desempeño superior a los modelos más chicos los hizo increíblemente\n",
    "útiles para los desarrolladores que arman aplicaciones con NLP. Estos modelos\n",
    "se pueden acceder vía la librería `transformers` de Hugging Face, vía OpenAI\n",
    "usando la librería `openai`, y vía Cohere usando la librería `cohere`.\n",
    "\n",
    "Pregunta: ¿Qué librerías están cerca de Buenos Aires?\n",
    "\n",
    "Respuesta (escribe como argentina informal): \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(llm_gpt3_5(prompt_argentino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "plantilla_colombiana = \"\"\"\n",
    "Responde a la pregunta con base en el siguiente contexto, parce. Si la\n",
    "pregunta no puede ser respondida con la información proporcionada, responde\n",
    "con \"No tengo ni idea, ome\".\n",
    "\n",
    "Contexto: Los Modelos de Lenguaje de Gran Escala (MLGEs) son lo último en modelos usados en el Procesamiento del Lenguaje Natural (NLP).\n",
    "Su desempeño superior a los modelos más chicos los hizo increíblemente\n",
    "útiles para los desarrolladores que arman aplicaciones con NLP. Estos modelos\n",
    "se pueden acceder vía la librería `transformers` de Hugging Face, vía OpenAI\n",
    "usando la librería `openai`, y vía Cohere usando la librería `cohere`.\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Respuesta (escribe como colombiano informal): \n",
    "\"\"\"\n",
    "\n",
    "prompt_plantilla_colombiana = PromptTemplate(\n",
    "    input_varibales = [\"query\"],\n",
    "    template = plantilla_colombiana\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/s0pfsfdn61b_3c4kn01_hktc0000gn/T/ipykernel_5028/4108755614.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_gpt3_5_chain = LLMChain(\n",
      "/var/folders/0z/s0pfsfdn61b_3c4kn01_hktc0000gn/T/ipykernel_5028/4108755614.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_gpt3_5_chain.run(pregunta)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Los LLMs son los Modelos de Lenguaje de Gran Escala, parce.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_gpt3_5_chain = LLMChain(\n",
    "    prompt = prompt_plantilla_colombiana,\n",
    "    llm = llm_gpt3_5\n",
    ")\n",
    "\n",
    "pregunta = \"Que son os LLMs\"\n",
    "\n",
    "llm_gpt3_5_chain.run(pregunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡El Vallenato es un género musical típico de la costa caribeña de Colombia, parce! ¡Es una mezcla de acordeón, caja y guacharaca que te pone a bailar al instante! ¡Una delicia, ome!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta = \"What is Vallenato\"\n",
    "\n",
    "llm_gpt3_5_chain.run(pregunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cadenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Son el corazon de langchain\n",
    "\n",
    "Secuencias de eslabones\n",
    "\n",
    "Retorna una repsuesta que usa un LLM\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
